{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Harshita <br>\n",
        "Roll Number: 102317003<br>\n",
        "SubGroup: 3Q11"
      ],
      "metadata": {
        "id": "XjhDwRegtjiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Generate a dataset with atleast seven highly correlated columns and a target variable.\n",
        "Implement Ridge Regression using Gradient Descent Optimization. Take different\n",
        "values of learning rate (such as 0.0001,0.001,0.01,0.1,1,10) and regularization\n",
        "parameter (10-15,10-10,10-5,10- 3,0,1,10,20). Choose the best parameters for which ridge\n",
        "regression cost function is minimum and R2_score is maximum.  "
      ],
      "metadata": {
        "id": "xzSC_YZStJqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlfKtVrcswRz",
        "outputId": "a66e1d16-b4b0-4e72-d6cc-665fd06683ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal LR, Lambda, R2 = (0.1, 0, 0.9920703853817209)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(0)\n",
        "N = 500\n",
        "z_val = np.random.randn(N)\n",
        "features = np.column_stack([z_val + 0.01 * np.random.randn(N) for _ in range(7)])\n",
        "features = np.column_stack([features, 0.5 * z_val + 0.2 * np.random.randn(N)])\n",
        "true_weights = np.array([3, -2, 1, 0, 0.5, -1, 2, 4], dtype=float)\n",
        "target = features.dot(true_weights) + 0.5 * np.random.randn(N)\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(features, target, test_size=0.25, random_state=1)\n",
        "\n",
        "mean_vals = X_tr.mean(axis=0)\n",
        "std_vals = X_tr.std(axis=0)\n",
        "std_vals = np.where(np.isfinite(std_vals) & (std_vals > 0), std_vals, 1.0)\n",
        "X_tr = (X_tr - mean_vals) / std_vals\n",
        "X_te = (X_te - mean_vals) / std_vals\n",
        "\n",
        "def ridge_reg(X, y, alpha, lam, iters=2000):\n",
        "    X = X.astype(np.float64); y = y.astype(np.float64)\n",
        "    m, n = X.shape\n",
        "    wts = np.zeros(n, dtype=np.float64)\n",
        "    bias = 0.0\n",
        "    for _ in range(iters):\n",
        "        preds = X.dot(wts) + bias\n",
        "        diff = preds - y\n",
        "        grad_wts = (2 / m) * (X.T.dot(diff)) + 2 * lam * wts\n",
        "        grad_bias = (2 / m) * diff.sum()\n",
        "        wts -= alpha * grad_wts\n",
        "        bias -= alpha * grad_bias\n",
        "        if not (np.isfinite(wts).all() and np.isfinite(bias)):\n",
        "            return None\n",
        "    return wts, bias\n",
        "\n",
        "alphas = [0.0001, 0.001, 0.01, 0.1]\n",
        "lambdas = [0, 0.001, 0.01, 0.1, 1, 10]\n",
        "top_r2 = -1\n",
        "opt_params = None\n",
        "\n",
        "for a in alphas:\n",
        "    for lam in lambdas:\n",
        "        outcome = ridge_reg(X_tr, y_tr, a, lam)\n",
        "        if outcome is None:\n",
        "            continue\n",
        "        wts, bias = outcome\n",
        "        preds = X_te.dot(wts) + bias\n",
        "        if not np.isfinite(preds).all():\n",
        "            continue\n",
        "        r2_val = r2_score(y_te, preds)\n",
        "        if r2_val > top_r2:\n",
        "            top_r2 = r2_val\n",
        "            opt_params = (a, lam, r2_val)\n",
        "\n",
        "print(\"Optimal LR, Lambda, R2 =\", opt_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Load the Hitters dataset from the following link\n",
        "https://drive.google.com/file/d/1qzCKF6JKKMB0p7ul_lLy8tdmRk3vE_bG/view?usp=sharing   <br>\n",
        "(a)\n",
        "Pre-process the data (null values, noise, categorical to numerical encoding)<br>\n",
        "(b) Separate input and output features and perform scaling <br>\n",
        "(c) Fit a Linear, Ridge (use regularization parameter as 0.5748), and LASSO (use\n",
        "regularization parameter as 0.5748) regression function on the dataset. <br>\n",
        "(d) Evaluate the performance of each trained model on test set. Which model\n",
        "performs the best and Why?"
      ],
      "metadata": {
        "id": "mn0Y3QiYu5CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"Hitters.csv\")\n",
        "data = data.dropna(subset=[\"Salary\"])\n",
        "data = data.fillna(data.median(numeric_only=True))\n",
        "for col in [\"League\", \"Division\", \"NewLeague\"]:\n",
        "    data[col] = data[col].astype(\"category\").cat.codes\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X = data.drop(\"Salary\", axis=1)\n",
        "y = data[\"Salary\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "model_lr = LinearRegression().fit(X_train, y_train)\n",
        "model_ridge = Ridge(alpha=0.5748).fit(X_train, y_train)\n",
        "model_lasso = Lasso(alpha=0.5748, max_iter=5000).fit(X_train, y_train)\n",
        "\n",
        "results = []\n",
        "for name, mdl in [(\"Linear\", model_lr), (\"Ridge\", model_ridge), (\"Lasso\", model_lasso)]:\n",
        "    preds = mdl.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, preds)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    results.append((name, mse, r2))\n",
        "    print(f\"{name}: MSE={mse:.2f}, R2={r2:.4f}\")\n",
        "\n",
        "best = max(results, key=lambda x: x[2])\n",
        "print(f\"\\nBest Model: {best[0]} (R2={best[2]:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1A72iYfvJnP",
        "outputId": "42260f71-1ba4-4c80-f40c-43a113d16cc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear: MSE=131898.53, R2=0.5532\n",
            "Ridge: MSE=128967.77, R2=0.5631\n",
            "Lasso: MSE=128572.34, R2=0.5644\n",
            "\n",
            "Best Model: Lasso (R2=0.5644)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV)\n",
        "function of Python. Implement both on Boston House Prediction Dataset (load_boston\n",
        "dataset from sklearn.datasets).  "
      ],
      "metadata": {
        "id": "XW0WoOn5wSGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "X = data.drop('medv', axis=1)\n",
        "y = data['medv']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "ridge = RidgeCV(alphas=alphas, cv=10)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "lasso = LassoCV(alphas=alphas, cv=10, max_iter=10000, random_state=42)\n",
        "lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "pred_ridge = ridge.predict(X_test_scaled)\n",
        "pred_lasso = lasso.predict(X_test_scaled)\n",
        "\n",
        "print(\"Ridge Alpha:\", ridge.alpha_, \"| R2:\", round(r2_score(y_test, pred_ridge), 4))\n",
        "print(\"Lasso Alpha:\", lasso.alpha_, \"| R2:\", round(r2_score(y_test, pred_lasso), 4))\n",
        "print(\"\\nRidge RMSE:\", round(np.sqrt(mean_squared_error(y_test, pred_ridge)), 2))\n",
        "print(\"Lasso RMSE:\", round(np.sqrt(mean_squared_error(y_test, pred_lasso)), 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skcSp25EwWFX",
        "outputId": "a181ad04-099a-4944-c8f7-f0814e2cb02d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Alpha: 10.0 | R2: 0.7073\n",
            "Lasso Alpha: 0.001 | R2: 0.7112\n",
            "\n",
            "Ridge RMSE: 4.67\n",
            "Lasso RMSE: 4.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Multiclass Logistic Regression: Implement Multiclass Logistic Regression (step-by step)\n",
        "on Iris dataset using one vs. rest strategy?"
      ],
      "metadata": {
        "id": "A0yNxkRPw3mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "ovr_model = OneVsRestClassifier(lr)\n",
        "ovr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ovr_model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "for i, clf in enumerate(ovr_model.estimators_):\n",
        "    print(f\"Class {i} vs Rest - Accuracy:\", round(clf.score(X_test, (y_test == i).astype(int)), 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zOWpPAYw5WJ",
        "outputId": "85d50a50-2009-476a-fa7d-916a63290a34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9737\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        15\n",
            "  versicolor       1.00      0.91      0.95        11\n",
            "   virginica       0.92      1.00      0.96        12\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.97      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "Class 0 vs Rest - Accuracy: 1.0\n",
            "Class 1 vs Rest - Accuracy: 0.7632\n",
            "Class 2 vs Rest - Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}